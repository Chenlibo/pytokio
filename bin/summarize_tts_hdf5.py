#!/usr/bin/env python
"""
Summarize the contents of a TOKIO TimeSeries (TTS) HDF5 file generated by
TimeSeries.commit_dataset().  This will eventually be merged with the
functionality provided by summarize_hdf5.py once the TTS HDF5 and pylmt formats
converge.
"""

import json
import math
import datetime
import argparse
import numpy
import tokio.connectors.hdf5

DIVISOR = 1024.0 # or 1000.0 for base-10

def humanize_units(byte_count):
    """
    Convert a raw byte count into human-readable base2 units
    """
    units = ["bytes", "KiB", "MiB", "GiB", "TiB"]
    result = byte_count
    index = 0
    while index < len(units) - 1:
        new_result = result / DIVISOR
        if new_result < 1.0:
            break
        else:
            index += 1
            result = new_result

    return result, units[index]

def convert_signed_zeroes(matrix, inverse=False):
    """
    Because we initialize datasets with -0.0, we can scan the sign bit of every
    element of an array to determine how many data were never populated.  This
    converts negative zeros to ones and all other data into zeros then count up
    the number of missing elements in the array.
    """
    if inverse:
        converter = numpy.vectorize(lambda x: 0 if math.copysign(1, x) < 0.0 else 1)
    else:
        converter = numpy.vectorize(lambda x: 1 if math.copysign(1, x) < 0.0 else 0)
    return converter(matrix)

def summarize_tts_hdf5(hdf5_file):
    """
    Generate summary data based on the contents of TOKIO timeseries HDF5 file
    """
    read_bytes = hdf5_file['/datatargets/readbytes'][:, :].sum()
    write_bytes = hdf5_file['/datatargets/writebytes'][:, :].sum()

    # readrates and writerates come via the same collectd message, so if one is
    # missing, both are missing
    values = hdf5_file['/datatargets/readbytes'][:, :]
    num_missing = convert_signed_zeroes(values).sum()
    total = values.shape[0] * values.shape[1]

    # find the row offset containing the first and last nonzero data
    first_time_idx = -1
    last_time_idx = -1
    nonzero_rows = convert_signed_zeroes(values, inverse=True).sum(axis=1)
    for index, value in enumerate(nonzero_rows):
        if first_time_idx < 0 and value > 0:
            first_time_idx = index
        if value > 0:
            last_time_idx = index

    return {
        'read_bytes': read_bytes,
        'write_bytes': write_bytes,
        'missing_pts': num_missing,
        'total_pts': total,
        'missing_pct': (100.0 * float(num_missing) / total),
        'first_nonzero_idx': first_time_idx,
        'last_nonzero_idx': last_time_idx,
    }

def print_timesteps(hdf5_file):
    """
    Print a summary of read/write bytes for each time step
    """
    timestamps = hdf5_file['/datatargets/timestamps'][:]
    read_bytes = hdf5_file['/datatargets/readbytes'][:, :].sum(axis=1)
    write_bytes = hdf5_file['/datatargets/writebytes'][:, :].sum(axis=1)

    for index, timestamp in enumerate(timestamps):
        print "%12s %14.2f read, %14.2f written" % (datetime.datetime.fromtimestamp(timestamp),
                                                    read_bytes[index],
                                                    write_bytes[index])

def print_columns(hdf5_file):
    """
    Print a summary of read/write bytes for each device
    """
    for index, column_name in enumerate(list(hdf5_file['/datatargets/readbytes'].attrs['columns'])):
        print "%12s %14.2f read, %14.2f written" % (
            column_name,
            hdf5_file['/datatargets/readbytes'][:, index].sum(),
            hdf5_file['/datatargets/writebytes'][:, index].sum())

def _summarize_tts_hdf5():
    """
    Summarize the contents of an HDF5 file generated by cache_collectdes_supplemental.py
    """
    parser = argparse.ArgumentParser()
    parser.add_argument("file", type=str, help="HDF5 file to summarize")
    parser.add_argument('-j', '--json', action='store_true', help='output as json')
    parser.add_argument('--timesteps', action='store_true', help='print a summary at each timestep')
    parser.add_argument('--columns', action='store_true', help='print a summary of each column')
    args = parser.parse_args()

    hdf5_file = tokio.connectors.hdf5.Hdf5(args.file, 'r')
    results = summarize_tts_hdf5(hdf5_file)
    if args.timesteps:
        print_timesteps(hdf5_file)
    if args.columns:
        print_columns(hdf5_file)

    if args.json:
        print json.dumps(results, indent=4, sort_keys=True)
    else:
        print "Data Read:            %5.1f %s" % humanize_units(results['read_bytes'])
        print "Data Written:         %5.1f %s" % humanize_units(results['write_bytes'])
        print "Missing data points:  %9d" % results['missing_pts']
        print "Expected data points: %9d" % results['total_pts']
        print "Percent data missing: %8.1f%%" % results['missing_pct']
        print "First non-empty row:  %9d" % results['first_nonzero_idx']
        print "Last non-empty row:   %9d" % results['last_nonzero_idx']

if __name__ == '__main__':
    _summarize_tts_hdf5()
