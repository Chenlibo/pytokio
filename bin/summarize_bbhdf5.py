#!/usr/bin/env python
"""
This is a proof-of-concept code to summarize the contents of an HDF5 file
generated by cache_collectdes_supplemental.py.  It should be converted to use
tokio.connectors.hdf5 once the TOKIOfile HDF5 schema is nailed down and
implemented correctly.
"""

import json
import math
import datetime
import argparse
import numpy
import h5py

DIVISOR = 1024.0 # or 1000.0 for base-10

def humanize_units(byte_count):
    units = [ "bytes", "KiB", "MiB", "GiB", "TiB", "PiB", "EiB" ]
    result = byte_count
    index = 0
    while index < len(units):
        new_result = result / DIVISOR
        if new_result < 1.0:
            break
        else:
            index += 1
            result = new_result

    return result, units[index]

def convert_signed_zeroes(matrix, inverse=False):
    """
    Because we initialize datasets with -0.0, we can scan the sign bit of every
    element of an array to determine how many data were never populated.  This
    converts negative zeros to ones and all other data into zeros then count up
    the number of missing elements in the array.
    """
    if inverse:
        converter = numpy.vectorize(lambda x: 0 if math.copysign(1, x) < 0.0 else 1)
    else:
        converter = numpy.vectorize(lambda x: 1 if math.copysign(1, x) < 0.0 else 0)
    return converter(matrix)

def summarize_bbhdf5(hdf5_file):
    timestep = hdf5_file['/bytes/timestamps'][1] - hdf5_file['/bytes/timestamps'][0]
    read_bytes = hdf5_file['/bytes/readrates'][:,:].sum() * timestep
    write_bytes = hdf5_file['/bytes/writerates'][:,:].sum() * timestep

    # readrates and writerates come via the same collectd message, so if one is
    # missing, both are missing
    values = hdf5_file['/bytes/readrates'][:,:]
    num_missing = convert_signed_zeroes(values).sum()
    total = values.shape[0] * values.shape[1]

    # find the row offset containing the first and last nonzero data
    first_time_idx = -1
    last_time_idx = -1
    nonzero_rows = convert_signed_zeroes(values, inverse=True).sum(axis=1)
    for index, value in enumerate(nonzero_rows):
        if first_time_idx < 0 and value > 0:
            first_time_idx = index
        if value > 0:
            last_time_idx = index

    return {
        'read_bytes': read_bytes,
        'write_bytes': write_bytes,
        'missing_pts': num_missing,
        'total_pts': total,
        'missing_pct': (100.0 * float(num_missing) / total),
        'first_nonzero_idx': first_time_idx,
        'last_nonzero_idx': last_time_idx,
    }

def print_timesteps(hdf5_file):
    """
    Print a summary of read/write bytes for each time step
    """
    timestamps = hdf5_file['/bytes/timestamps'][:]
    timestep = timestamps[1] - timestamps[0]
    read_bytes = hdf5_file['/bytes/readrates'][:,:].sum(axis=1) * timestep
    write_bytes = hdf5_file['/bytes/writerates'][:,:].sum(axis=1) * timestep

    for index, timestamp in enumerate(timestamps):
        print "%12s %14.2f read, %14.2f written" % (datetime.datetime.fromtimestamp(timestamp), read_bytes[index], write_bytes[index])

def print_columns(hdf5_file):
    """
    Print a summary of read/write bytes for each device
    """
    timestep = hdf5_file['/bytes/timestamps'][1] - hdf5_file['/bytes/timestamps'][0]
    for index, column_name in enumerate(list(hdf5_file['/bytes/readrates'].attrs['columns'])):
        print "%12s %14.2f read, %14.2f written" % (
            column_name,
            hdf5_file['/bytes/readrates'][:, index].sum() * timestep,
            hdf5_file['/bytes/writerates'][:, index].sum() * timestep)

def _summarize_bbhdf5():
    """
    Summarize the contents of an HDF5 file generated by cache_collectdes_supplemental.py
    """
    parser = argparse.ArgumentParser()
    parser.add_argument("file", type=str, help="HDF5 file to summarize")
    parser.add_argument('-j', '--json', action='store_true', help='output as json')
    parser.add_argument('--timesteps', action='store_true', help='print a summary at each timestep')
    parser.add_argument('--columns', action='store_true', help='print a summary of each column')
    args = parser.parse_args()

    hdf5_file = h5py.File(args.file, 'r')
    results = summarize_bbhdf5(hdf5_file)
    if args.timesteps:
        print_timesteps(hdf5_file)
    if args.columns:
        print_columns(hdf5_file)

    if args.json:
        print json.dumps(results, indent=4, sort_keys=True)
    else:
        print "Data Read:            %5.1f %s" % humanize_units(results['read_bytes'])
        print "Data Written:         %5.1f %s" % humanize_units(results['write_bytes'])
        print "Missing data points:  %9d" % results['missing_pts']
        print "Expected data points: %9d" % results['total_pts']
        print "Percent data missing: %8.1f%%" % results['missing_pct']
        print "First non-empty row:  %9d" % results['first_nonzero_idx']
        print "Last non-empty row:   %9d" % results['last_nonzero_idx']

if __name__ == '__main__':
    _summarize_bbhdf5()
