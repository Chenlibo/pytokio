#!/usr/bin/env python
"""Connect to Darshan logs.

This connector provides an interface into Darshan logs created by Darshan 3.0 or
higher and represents the counters and data contained therein as a Python
dictionary.  This dictionary has the following structure, where ``block``
denote literal key names.

* ``header`` which contains key-value pairs corresponding to each line in the
  header.  ``exe`` and ``metadata`` are lists; the other keys correspond to a
  single scalar value.

  * ``compression``, ``end_time``, ``end_time_string``, ``exe``, etc

* ``counters``

  * `modulename` which is ``posix``, ``lustre``, ``stdio``, etc

    * `recordname`, which is usually the full path to a file opened by the
      profiled application _or_ ``_perf`` (contains performance summary metrics)
      or ``_total`` (contains aggregate file statistics)

      * ranknum which is a string (``0``, ``1``, etc or ``-1``)

        * `counternames`, which depends on the Darshan module defined by
          `modulename` above

* ``mounts`` which is the mount table with keys of a path to a mount location
  and values of the file system type

The `counternames` are module-specific and have their module name prefix
stripped off.  The following counter names are examples of what a Darshan log
may expose through this connector for the ``posix`` module:

* ``BYTES_READ`` and ``BYTES_WRITTEN`` - number of bytes read/written to the file
* ``MAX_BYTE_WRITTEN`` and ``MAX_BYTE_READ`` - highest byte written/read; useful if an application re-reads or re-writes a lot of data
* ``WRITES`` and ``READS`` - number of write and read ops issued
* ``F_WRITE_TIME`` and ``F_READ_TIME`` - amount of time spent inside write and read calls (in seconds)
* ``F_META_TIME`` - amount of time spent in metadata (i.e., non-read/write) calls

Similarly the ``lustre`` module provides the following counter keys:

* ``MDTS`` - number of MDTs in the underlying file system
* ``OSTS`` - number of OSTs in the underlying file system
* ``OST_ID_0`` - the OBD index for the 0th OST over which the file is striped
* ``STRIPE_OFFSET`` - the setting used to define stripe offset when the file was created
* ``STRIPE_SIZE`` - the size, in bytes, of each stripe
* ``STRIPE_WIDTH`` - how many OSTs the file touches

Note:
    This connector presently relies on ``darshan-parser`` to convert the binary
    logs to ASCII, then convert the ASCII into Python objects.  In the future,
    we plan on using the Python API provided by darshan-utils to circumvent the
    ASCII translation.
"""

import os
import re
import json
import errno
import warnings
import subprocess

DARSHAN_PARSER_BIN = 'darshan-parser'

class Darshan(dict):
    """Dictionary sublcass that self-populates with Darshan log data.
    """

    def __init__(self, log_file=None, cache_file=None, silent_errors=False):
        """Initialize the object from either a Darshan log or a cache file.

        Configures the object's internal state to either operate on a Darshan
        log file or a cached JSON representation of a previously processed
        Darshan log.

        Args:
            log_file (str, optional): Path to a Darshan log to be processed
            cache_file (str, optional): Path to a Darshan log's contents cached
                as JSON
            silent_errors (bool): If True, suppress the stderr of the
                ``darshan-parser`` subprocess when running.

        Attributes:
            cache_file (str): Path to the JSON cache file to load
            log_file (str): Path to the Darshan log file to load
            silent_errors (bool): Squelch errors generated by
                ``darshan-parser`` subprocess if True
        """
        super(Darshan, self).__init__(self)
        self.cache_file = cache_file
        self.log_file = log_file
        self.silent_errors = silent_errors
        if self.cache_file is None and self.log_file is None:
            raise Exception("parameters should be provided (at least log_file or cache_file)")
        elif self.cache_file:
            self.__setitem__(json.load(cache_file))

    def __repr__(self):
        """Serialize self into JSON.

        Returns:
            str: JSON representation of the object
        """
        return json.dumps(self.values())

    def save_cache(self, output_file=None):
        """Serialize self into a JSON output file.

        Args:
            output_file (str, optional): Path to the file to which JSON
                representation of object should be stored.  If None, print to
                stdout.
        """
        if output_file is None:
            print json.dumps(self)
        else:
            with open(output_file, 'w') as output:
                output.write(json.dumps(self))

    def darshan_parser_base(self):
        """Populate data produced by ``darshan-parser --base``

        Runs the ``darshan-parser --base`` and convert all results into
        key-value pairs which are inserted into the object.

        Returns:
            dict: Dictionary containing all key-value pairs generated by running
            ``darshan-parser --base``.  These values are also accessible via the
            `BASE` key in the object.
        """
        return self._darshan_parser("BASE")

    def darshan_parser_total(self):
        """Populate data produced by ``darshan-parser --total``

        Runs the ``darshan-parser --total`` and convert all results into
        key-value pairs which are inserted into the object.

        Returns:
            dict: Dictionary containing all key-value pairs generated by running
            ``darshan-parser --total``.  These values are also accessible via
            the `TOTAL` key in the object.
        """
        return self._darshan_parser("TOTAL")

    def darshan_parser_perf(self):
        """Populate data produced by ``darshan-parser --perf``

        Runs the ``darshan-parser --perf`` and convert all results into
        key-value pairs which are inserted into the object.

        Returns:
            dict: Dictionary containing all key-value pairs generated by running
            ``darshan-parser --perf``.  These values are also accessible via the
            `PERF` key in the object.
        """
        return self._darshan_parser("PERF")

    def _darshan_parser(self, counter_flag="BASE"):
        """Run ``darshan-parser`` and parse its output

        Calls ``darshan-parser`` with ``--base``, ``--total``, or ``--perf`` on
        the log file given by the `log_file` attribute and walk its output
        to identify different modules' sections.  For each section, this
        function dispatches a section-specific parser.

        Args:
            counter_flag (str): ``BASE``, ``TOTAL``, or ``PERF``

        Returns:
            Darshan: Returns self after updating keys/values based on the parsed
            results.
        """

        def counter_parser(line, counter_flag):
            """Dispatch a specific ``darshan-parser`` option based

            Args:
                counter_flag (str): ``BASE``, ``TOTAL``, or ``PERF``

            Returns:
                tuple: Returns data specific to the `counter_flag` specified or
                None if an invalid `counter_flag` is given.
            """
            if counter_flag == "BASE":
                return parse_base_counters(line)
            if counter_flag == "PERF":
                return parse_perf_counters(line)
            if counter_flag == "TOTAL":
                return parse_total_counters(line)
            return None

        def is_valid_counter(counter, line):
            """Determine if a line contains counter info or a new module

            * If counter is not None, this line is valid (return True)
            * If counter is None but we can identify a module section, return it
            * If counter is None but we cannot identify a module section, return False
            """
            if counter is None:
                match = module_rex.search(line)
                if match is not None:
                    module_section = match.group(1)
                    module_section = module_section.replace('-', '') # because of "MPI-IO" and "MPIIO"
                    module_section = module_section.replace('/', '') # because of "BG/Q" and "BGQ"
                    return False, module_section
                return False, None
            return True, None

        def insert_record(section, module, file_name, rank, counter, value, counter_prefix=None):
            """
            Embed a counter=value pair deep within the darshan_data structure based
            on a bunch of nested keys.
            """
            # Force the local shadow of 'module' to lowercase
            module = module.lower()
            # Assert that the counter actually belongs to the current module
            if counter_prefix is not None:
                if counter.startswith(counter_prefix):
                    # Strip off the counter_prefix from the counter name
                    counter = counter[len(counter_prefix):]
                else:
                    raise Exception("counter %s does not start with prefix %s" % (counter, counter_prefix))

            # Otherwise insert the record--this logic should be made more flexible
            if section not in self:
                self[section] = {}
            if module not in self[section]:
                self[section][module] = {}
            if file_name not in self[section][module]:
                self[section][module][file_name] = {}
            if rank is None:
                insert_base = self[section][module][file_name]
            else:
                if rank not in self[section][module][file_name]:
                    self[section][module][file_name][rank] = {}
                insert_base = self[section][module][file_name][rank]

            if counter in insert_base:
                raise Exception("Duplicate counter %s found in %s->%s->%s (rank=%s)" % (counter, section, module, file_name, rank))
            else:
                if '.' in value:
                    value = float(value)
                else:
                    value = long(value)
                insert_base[counter] = value

        if self.log_file is None:
            return self

        section = None
        counter = None
        module_section = None
        # This regex must match every possible module name
        module_rex = re.compile(r'^# ([A-Z\-0-9/]+) module data\s*$')
        if counter_flag in ["BASE", "TOTAL", "PERF"]:
            darshan_flag = "--" + counter_flag.lower()
        else:
            darshan_flag = ""

        cmd = [DARSHAN_PARSER_BIN, darshan_flag, self.log_file]
        try:
            if self.silent_errors:
                with open(os.devnull, 'w') as devnull:
                    output_str = subprocess.check_output(cmd, stderr=devnull)
            else:
                output_str = subprocess.check_output(cmd)
        except subprocess.CalledProcessError as error:
            warnings.warn("darshan-parser returned nonzero exit code (%d)" % error.returncode)
            output_str = error.output
        except OSError as error:
            if error[0] == errno.ENOENT:
                raise type(error)(error[0], "darshan-parser command not found")
            raise


        for line in output_str.splitlines():
            # Is this the start of a new section?
            # Why do we look at section, refactorize failed
            if section is None and line.startswith("# darshan log version:"):
                section = "header"
                if section not in self.keys():
                    self[section] = {}

            elif section == "header" and line.startswith("# mounted file systems"):
                section = "mounts"
                if section not in self.keys():
                    self[section] = {}

            elif section == "mounts" and line.startswith("# **********************"):  # understand the utility of these stars
                section = "counters"
                if section not in self.keys():
                    self[section] = {}

            # otherwise use the appropriate parser for this section
            if section == "header":
                key, val = parse_header(line)
                if key is None:
                    pass
                elif key == "metadata":
                    if key not in self[section]:
                        self[section][key] = []
                    self[section][key].append(val)
                else:
                    self[section][key] = val
            elif section == 'mounts':
                key, val = parse_mounts(line)
                if key is not None:
                    self[section][key] = val

            elif section == 'counters':
                if counter_flag == "BASE":
                    _, rank, _, counter, value, file_name, _, _ = counter_parser(line, counter_flag)
                    if module_section is not None:
                        # If it is none, is_valid_counter check below will bail
                        counter_prefix = module_section + "_"
                elif counter_flag == "TOTAL":
                    counter, value = counter_parser(line, counter_flag)
                    file_name = '_total'
                    rank = None
                    if module_section is not None:
                        counter_prefix = 'total_%s_' % module_section
                elif counter_flag == "PERF":
                    counter, value = counter_parser(line, counter_flag)
                    file_name = '_perf'
                    rank = None
                    counter_prefix = None
                else:
                    continue

            # If no valid counter found, is this the start of a new module?
            valid, new_module_section = is_valid_counter(counter, line)
            if new_module_section is not None:
                module_section = new_module_section
            if valid is False:
                continue
            # Reminder: section is already defined as the global parser state
            insert_record(section=section,
                          module=module_section,
                          file_name=file_name,
                          rank=rank,
                          counter=counter,
                          value=value,
                          counter_prefix=counter_prefix)
        return self

def parse_header(line):
    """Parse the header lines of ``darshan-parser``.

    Accepts a line that may or may not be a header line as printed by
    ``darshan-parser``.  Such header lines take the form::

        # darshan log version: 3.10
        # compression method: ZLIB
        # exe: /home/user/bin/myjob.exe --whatever
        # uid: 69615

    If it is a valid header line, return a key-value pair corresponding to
    its decoded contents.

    Args:
        line (str): A single line of output from ``darshan-parser``

    Returns:
        tuple: Returns a (key, value) corresponding to the key and value
        decoded from the header `line`, or ``(None, None)`` if the line does
        not appear to contain a known header field.
    """
    key, val = None, None
    if line.startswith("# darshan log version:"):
        key, val = 'version', line.split()[-1]
    elif line.startswith("# compression method:"):
        key, val = "compression", line.split()[-1]
    elif line.startswith("# exe:"):
        key, val = "exe", line.split()[2:]
    elif line.startswith("# uid:"):
        key, val = "uid", int(line.split()[-1])
    elif line.startswith("# jobid:"):
        key, val = 'jobid', line.split()[-1]
    elif line.startswith("# start_time:"):
        key, val = 'start_time', long(line.split()[2])
    elif line.startswith("# start_time_asci:"):
        key, val = 'start_time_string', line.split(None, 2)[-1].strip()
    elif line.startswith("# end_time:"):
        key, val = 'end_time', long(line.split()[2])
    elif line.startswith("# end_time_asci:"):
        key, val = 'end_time_string', line.split(None, 2)[-1].strip()
    elif line.startswith("# nprocs:"):
        key, val = "nprocs", int(line.split()[-1])
    elif line.startswith("# run time:"):
        key, val = "walltime", int(line.split()[-1])
    elif line.startswith("# metadata:"):
        key, val = "metadata", line.split(None, 2)[-1].strip()
    return key, val


def parse_mounts(line):
    """Parse a mount table line from ``darshan-parser``.

    Accepts a line that may or may not be a mount table entry from
    ``darshan-parser``.  Such lines take the form::

        # mount entry:  /usr/lib64/libibverbs.so.1.0.0  dvs

    If `line` is a valid mount table entry, return a key-value
    representation of its contents.

    Args:
        line (str): A single line of output from ``darshan-parser``

    Returns:
        tuple: Returns a (key, value) corresponding to the mount table
        entry, or ``(None, None)`` if the line is not a valid mount
        table entry.
    """
    if line.startswith("# mount entry:\t"):
        key, val = line.split('\t')[1:3]
        return key, val.strip()
    return None, None

def parse_base_counters(line):
    """Parse a counter line from ``darshan-parser --base``.

    Parse a line containing counter data.  Such lines are tab-delimited and
    take the form::

        module, rank, record_id, counter, value, file_name, mount_pt, fs_type

    Args:
        line (str): A single line of output from ``darshan-parser --base``

    Returns:
        tuple: Returns a tuple containing eight values, each corresponding
        to a field represented in `line`.  If `line` is not a valid counter
        line, all values will be None.
    """
    if not line.startswith("#"):
        args = line.split('\t')
        if len(args) == 8:
            return tuple(args)
    return None, None, None, None, None, None, None, None

def parse_total_counters(line):
    """Parse a counter line from ``darshan-parser --total``.

    Parse a line containing counter data from ``darshan-parser --total``.
    Such lines are of the form::

        total_MPIIO_F_READ_END_TIMESTAMP: 0.000000

    Args:
        line (str): A single line of output from ``darshan-parser --total``

    Returns:
        tuple: Returns a single (key, value) pair corresponding to a
        counted metric and its total value.  If `line` is not a valid
        counter line, ``(None, None)`` are returned.
    """
    if not line.startswith("#"):
        args = line.split(':')
        if len(args) == 2:
            return args[0].strip(), args[1].strip()
    return None, None

def parse_perf_counters(line):
    """Parse a counter line from ``darshan-parser --perf``.

    Parse a line containing counter data from ``darshan-parser --perf``.
    Such lines look like::

        # total_bytes: 2199023259968
        # unique files: slowest_rank_io_time: 0.000000
        # shared files: time_by_cumul_io_only: 39.992327
        # agg_perf_by_slowest: 28670.996545

    Args:
        line (str): A single line of output from ``darshan-parser --perf``

    Returns:
        tuple: Returns a single (key, value) pair corresponding to the
        performance metric encoded in `line`.  If `line` is not a valid
        performance counter line, ``(None, None)`` is returned.
    """
    if line.startswith('# total_bytes:') or line.startswith('# agg_perf_by'):
        key, value = line[2:].split(':')
    elif line.startswith('# unique files:') or line.startswith('# shared files:'):
        key_suffix, key, value = line[2:].split(':')
        key += '_%s' % key_suffix.replace(' ', '_')
    else:
        return None, None

    return key.strip(), value.strip()
