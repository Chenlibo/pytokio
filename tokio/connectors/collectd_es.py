#!/usr/bin/env python
"""
Retrieve data generated by collectd and stored in ElasticSearch
"""

import copy
import time
try:
    from elasticsearch import Elasticsearch
except ImportError:
    pass

class CollectdEs(object):
    """
    Wrapper around an ElasticSearch connection context that provides simpler
    scrolling functionality for very long documents and callback functions to be
    run after each page is retrieved.
    """
    def __init__(self, host, port, index=None, scroll_size='1m', page_size=10000, timeout=30):
        """
        Store all the settings that will be used to instantiate an ElasticSearch connection object
        """
        # retain state of ElasticSearch client
        self.client = None
        self.page = None
        self.scroll_pages = []
        self.index = index
        # connection info
        self.connect_host = host
        self.connect_port = port
        self.connect_timeout = timeout
        # for the scroll API
        self.page_size = page_size
        self.scroll_size = scroll_size
        self.scroll_id = None
        # for query_and_scroll
        self.num_flushes = 0
        # hidden parameters to refine how ElasticSearch queries are issued
        self.sort_by = ''

        self.connect()

    def connect(self):
        """
        Wrapper to instantiate a connection and retain the connection object
        """
        self.client = Elasticsearch(host=self.connect_host,
                                    port=self.connect_port,
                                    timeout=self.connect_timeout)

    def close(self):
        """
        Wrapper to close and invalidate the connection context
        """
        if self.client:
            self.client = None

    def query(self, query):
        """
        Issue an ElasticSearch query
        """
        self.page = self.client.search(body=query, index=self.index, sort=self.sort_by)
        if '_scroll_id' in self.page:
            self.scroll_id = self.page['_scroll_id']
        return self.page

    def scroll(self):
        """
        Issue a follow-up query for a query whose results didn't fall fit in a
        single return page
        """
        if self.scroll_id is None:
            raise Exception('no scroll id')
        self.page = self.client.scroll(scroll_id=self.scroll_id, scroll=self.scroll_size)
        return self.page

    def query_and_scroll(self, query, source_filter=True, filter_function=None,
                         flush_every=None, flush_function=None):
        """
        Issue a query and retain all results.  Optional arguments:
            source_filter - True to return all fields contained in each
                            document's _source field; otherwise, a list of
                            _source fields to include
            filter_function - function to call before each set of results is
                              appended to self.scroll_pages; if specified,
                              return value of this function is what is appended
            flush_every - trigger the flush function once the number of docs
                          contained across all self.scroll_pages reaches this
                          value
            flush_function - function to call when self.flush_every docs are
                             retrieved
        """

        def process_page(scroll_state):
            """
            Pull down a page, lightly filter it, and attach it to this object's page list
            """
            # nonlocal is Python 3 only...
            # nonlocal total_hits
            # nonlocal num_hits_since_flush

            if len(self.page['hits']['hits']) == 0:
                return False

            self.scroll_id = self.page['_scroll_id']
            num_hits = len(self.page['hits']['hits'])
            scroll_state['total_hits'] += num_hits

            # if this page will push us over flush_every, flush it first
            if flush_function is not None \
            and flush_every \
            and (scroll_state['num_hits_since_flush'] + num_hits) > flush_every:
                flush_function(self)
                scroll_state['num_hits_since_flush'] = 0
                self.num_flushes += 1

            # increment hits since flush only after we've (possibly) flushed
            scroll_state['num_hits_since_flush'] += num_hits

            # if a filter function exists, use its output as the page to append
            if filter_function is None:
                filtered_page = self.page
            else:
                filtered_page = filter_function(self.page)

            # finally append the page
            self.scroll_pages.append(filtered_page)
            return True

        # initialize the scroll state
        self.scroll_pages = []
        # note that we use a dict here because we need to manipulate these
        # values from within a nested function, and Python's scoping rules are
        # rather arbitrary and there's no way to pass individual variables by
        # reference
        scroll_state = {
            'total_hits': 0,
            'num_hits_since_flush': 0
        }

        # Get first set of results and a scroll id
        self.page = self.client.search(
            index=self.index,
            body=query,
            scroll=self.scroll_size,
            size=self.page_size,
            _source=source_filter,
        )

        more_results = process_page(scroll_state)

        while more_results:
            self.page = self.scroll()
            more_results = process_page(scroll_state)

def build_timeseries_query(orig_query, start, end):
    """
    Given a query dict and a start/end datetime object, return a new query
    object with the correct time ranges bounded.  Relies on orig_query
    containing at least one @timestamp field to indicate where the time ranges
    should be inserted.
    """
    def map_item(obj, target_key, map_function):
        """
        Recursively walk a hierarchy of dicts and lists, searching for a
        matching key.  For each match found, apply map_function to that key's
        value.
        """
        if isinstance(obj, list):
            iterator = enumerate
            if target_key in obj:
                return obj[target_key]
        elif isinstance(obj, dict):
            iterator = dict.iteritems
            if target_key in obj:
                return obj[target_key]
        else:
            # hit a dead end without a match
            return None
        # if this isn't a dead end, search down each iterable element
        for _, value in iterator(obj):
            if isinstance(value, (list, dict)):
                # dive down any discovered rabbit holes
                item = map_item(value, target_key, map_function)
                if item is not None:
                    map_function(item)
        return None

    def set_time_range(time_range_obj, time_format="epoch_second"):
        """
        Set the upper and lower bounds of a time range
        """
        time_range_obj['gte'] = long(time.mktime(start.timetuple()))
        time_range_obj['lt'] = long(time.mktime(end.timetuple()))
        time_range_obj['format'] = time_format

    query = copy.deepcopy(orig_query)

    map_item(query, '@timestamp', set_time_range)

    return query
